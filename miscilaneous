WITH cohort AS (
  SELECT
    start_time,
    COUNT(DISTINCT CASE WHEN period_number = 0 THEN person_id END) AS week_0,
    COUNT(DISTINCT CASE WHEN period_number = 1 THEN person_id END) AS week_1,
    COUNT(DISTINCT CASE WHEN period_number = 2 THEN person_id END) AS week_2,
    COUNT(DISTINCT CASE WHEN period_number = 3 THEN person_id END) AS week_3,
    COUNT(DISTINCT CASE WHEN period_number = 4 THEN person_id END) AS week_4,
    COUNT(DISTINCT CASE WHEN period_number = 5 THEN person_id END) AS week_5,
    COUNT(DISTINCT CASE WHEN period_number = 6 THEN person_id END) AS week_6,
    -- Repeat the pattern till week 52
    -- ...
    COUNT(DISTINCT CASE WHEN period_number = 52 THEN person_id END) AS week_52
  FROM (
    SELECT
      person_id,
      DATE_TRUNC('week', MIN(start)) AS start_time,
      DATE_DIFF('week', MIN(start), DATE_TRUNC('week', start)) AS period_number
    FROM histories
    GROUP BY person_id, DATE_TRUNC('week', start)
  ) AS cohort_data
  GROUP BY start_time
)

SELECT
  week_1 AS joined_week_1,
  COUNT(DISTINCT CASE WHEN week_2 > 0 THEN person_id END) AS visited_week_2,
  COUNT(DISTINCT CASE WHEN week_3 > 0 THEN person_id END) AS visited_week_3,
  COUNT(DISTINCT CASE WHEN week_4 > 0 THEN person_id END) AS visited_week_4,
  COUNT(DISTINCT CASE WHEN week_5 > 0 THEN person_id END) AS visited_week_5,
  COUNT(DISTINCT CASE WHEN week_6 > 0 THEN person_id END) AS visited_week_6,
  -- Repeat the columns for week 7 to week 52
  -- ...
  COUNT(DISTINCT CASE WHEN week_52 > 0 THEN person_id END) AS visited_week_52
FROM cohort
GROUP BY week_1
ORDER BY week_1;






SUM(CASE WHEN period_number = 0 THEN 1 ELSE 0 END) AS week0,
SUM(CASE WHEN period_number = 1 THEN 1 ELSE 0 END) AS week1,
SUM(CASE WHEN period_number = 2 THEN 1 ELSE 0 END) AS week2,
SUM(CASE WHEN period_number = 3 THEN 1 ELSE 0 END) AS week3,
SUM(CASE WHEN period_number = 5 THEN 1 ELSE 0 END) AS week5,
SUM(CASE WHEN period_number = 6 THEN 1 ELSE 0 END) AS week6,
SUM(CASE WHEN period_number = 7 THEN 1 ELSE 0 END) AS week7,
SUM(CASE WHEN period_number = 8 THEN 1 ELSE 0 END) AS week8,
SUM(CASE WHEN period_number = 9 THEN 1 ELSE 0 END) AS week9,
SUM(CASE WHEN period_number = 10 THEN 1 ELSE 0 END) AS week10,
SUM(CASE WHEN period_number = 11 THEN 1 ELSE 0 END) AS week11,
SUM(CASE WHEN period_number = 12 THEN 1 ELSE 0 END) AS week12,
SUM(CASE WHEN period_number = 13 THEN 1 ELSE 0 END) AS week13,
SUM(CASE WHEN period_number = 14 THEN 1 ELSE 0 END) AS week14,
SUM(CASE WHEN period_number = 15 THEN 1 ELSE 0 END) AS week15,
SUM(CASE WHEN period_number = 16 THEN 1 ELSE 0 END) AS week16,
SUM(CASE WHEN period_number = 17 THEN 1 ELSE 0 END) AS week17,
SUM(CASE WHEN period_number = 18 THEN 1 ELSE 0 END) AS week18,
SUM(CASE WHEN period_number = 19 THEN 1 ELSE 0 END) AS week19,
SUM(CASE WHEN period_number = 20 THEN 1 ELSE 0 END) AS week20,
SUM(CASE WHEN period_number = 21 THEN 1 ELSE 0 END) AS week21,
SUM(CASE WHEN period_number = 22 THEN 1 ELSE 0 END) AS week22,
SUM(CASE WHEN period_number = 23 THEN 1 ELSE 0 END) AS week23,
SUM(CASE WHEN period_number = 24 THEN 1 ELSE 0 END) AS week24,
SUM(CASE WHEN period_number = 25 THEN 1 ELSE 0 END) AS week25,
SUM(CASE WHEN period_number = 26 THEN 1 ELSE 0 END) AS week26,
SUM(CASE WHEN period_number = 27 THEN 1 ELSE 0 END) AS week27,
SUM(CASE WHEN period_number = 28 THEN 1 ELSE 0 END) AS week28,
SUM(CASE WHEN period_number = 29 THEN 1 ELSE 0 END) AS week29,
SUM(CASE WHEN period_number = 30 THEN 1 ELSE 0 END) AS week30,
SUM(CASE WHEN period_number = 31 THEN 1 ELSE 0 END) AS week31,
SUM(CASE WHEN period_number = 32 THEN 1 ELSE 0 END) AS week32,
SUM(CASE WHEN period_number = 33 THEN 1 ELSE 0 END) AS week33,
SUM(CASE WHEN period_number = 34 THEN 1 ELSE 0 END) AS week34,
SUM(CASE WHEN period_number = 35 THEN 1 ELSE 0 END) AS week35,
SUM(CASE WHEN period_number = 36 THEN 1 ELSE 0 END) AS week36,
SUM(CASE WHEN period_number = 37 THEN 1 ELSE 0 END) AS week37,
SUM(CASE WHEN period_number = 38 THEN 1 ELSE 0 END) AS week38,
SUM(CASE WHEN period_number = 39 THEN 1 ELSE 0 END) AS week39,
SUM(CASE WHEN period_number = 40 THEN 1 ELSE 0 END) AS week40,
SUM(CASE WHEN period_number = 41 THEN 1 ELSE 0 END) AS week41,
SUM(CASE WHEN period_number = 42 THEN 1 ELSE 0 END) AS week42,
SUM(CASE WHEN period_number = 43 THEN 1 ELSE 0 END) AS week43,
SUM(CASE WHEN period_number = 44 THEN 1 ELSE 0 END) AS week44,
SUM(CASE WHEN period_number = 45 THEN 1 ELSE 0 END) AS week45,
SUM(CASE WHEN period_number = 46 THEN 1 ELSE 0 END) AS week46,
SUM(CASE WHEN period_number = 47 THEN 1 ELSE 0 END) AS week47,
SUM(CASE WHEN period_number = 48 THEN 1 ELSE 0 END) AS week48,
SUM(CASE WHEN period_number = 49 THEN 1 ELSE 0 END) AS week49,
SUM(CASE WHEN period_number = 50 THEN 1 ELSE 0 END) AS week50,
SUM(CASE WHEN period_number = 51 THEN 1 ELSE 0 END) AS week51,
SUM(CASE WHEN period_number = 52 THEN 1 ELSE 0 END) AS week52








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Data Cleaning and Formatting
df["yyyymmdd"] = pd.to_datetime(df["yyyymmdd"])

# Summary Statistics
traffic_stats = df["traffic"].describe()
grouped_stats = df.groupby("hour")["traffic"].agg(["mean", "median", "std"])

# Data Visualization
# Line Plot
plt.figure(figsize=(12, 6))
sns.lineplot(x="yyyymmdd", y="traffic", data=df)
plt.title("Traffic Trend over Time")
plt.xlabel("Date")
plt.ylabel("Traffic")
plt.xticks(rotation=45)
plt.show()

# Histogram
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x="traffic", bins=20)
plt.title("Distribution of Traffic")
plt.xlabel("Traffic")
plt.ylabel("Count")
plt.show()

# Box Plot
plt.figure(figsize=(8, 6))
sns.boxplot(data=df, x="hour", y="traffic")
plt.title("Traffic Distribution by Hour")
plt.xlabel("Hour")
plt.ylabel("Traffic")
plt.show()

# Heatmap
hourly_traffic = df.pivot_table(index="hour", columns="yyyymmdd", values="traffic", aggfunc=np.mean)
plt.figure(figsize=(12, 6))
sns.heatmap(hourly_traffic, cmap="YlGnBu", linewidths=0.5)
plt.title("Hourly Traffic Heatmap")
plt.xlabel("Date")
plt.ylabel("Hour")
plt.xticks(rotation=45)
plt.show()

# Time-based Analysis
# Example: Rolling Average
df["rolling_avg"] = df.groupby("yyyymmdd")["traffic"].rolling(window=3, min_periods=1).mean()
plt.figure(figsize=(12, 6))
sns.lineplot(x="yyyymmdd", y="rolling_avg", data=df)
plt.title("Rolling Average of Traffic")
plt.xlabel("Date")
plt.ylabel("Rolling Average")
plt.xticks(rotation=45)
plt.show()

# Correlation Analysis
correlation_matrix = df.corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="YlGnBu")
plt.title("Correlation Matrix")
plt.show()

# Outlier Detection
Q1 = df.groupby("hour")["traffic"].transform(lambda x: x.quantile(0.25))
Q3 = df.groupby("hour")["traffic"].transform(lambda x: x.quantile(0.75))
IQR = Q3 - Q1
lower_fence = Q1 - 1.5 * IQR
upper_fence = Q3 + 1.5 * IQR
outliers = df[(df["traffic"] < lower_fence) | (df["traffic"] > upper_fence)]

plt.figure(figsize=(12, 6))
sns.scatterplot(x="yyyymmdd", y="traffic", data=df, label="Data")
sns.scatterplot(x="yyyymmdd", y="traffic", data=outliers, color="red", label="Outliers")
plt.title("Traffic with Outliers")
plt.xlabel("Date")
plt.ylabel("Traffic")
plt.xticks(rotation=45)
plt.legend()
plt.show()

# Seasonality and Trends (Time Series Decomposition)
from statsmodels.tsa.seasonal import seasonal_decompose

decomposition = seasonal_decompose(df.set_index("yyyymmdd")["traffic"], model="additive", period=24)

